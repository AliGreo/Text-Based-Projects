{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aligreualihassan/finetune-llama-2-qa-dataset?scriptVersionId=171727628\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q datasets peft trl bitsandbytes accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import the required packages\nfrom transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nimport pandas as pd \nimport numpy as np\nimport torch\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T18:13:09.30781Z","iopub.execute_input":"2024-04-12T18:13:09.308488Z","iopub.status.idle":"2024-04-12T18:13:18.355221Z","shell.execute_reply.started":"2024-04-12T18:13:09.308456Z","shell.execute_reply":"2024-04-12T18:13:18.354092Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#load the dataset from huggingface website\ndataset = load_dataset(\"locuslab/TOFU\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:13:18.357042Z","iopub.execute_input":"2024-04-12T18:13:18.357708Z","iopub.status.idle":"2024-04-12T18:13:20.282861Z","shell.execute_reply.started":"2024-04-12T18:13:18.357674Z","shell.execute_reply":"2024-04-12T18:13:20.281808Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 4000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#define the bits and bytes configration\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                               bnb_4bit_quant_type=\"nf4\",\n                               bnb_4bit_qunat_dtype=torch.float16,\n                               bnb_4bit_use_double_quant=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the model and tokenizer\n\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllama_model = AutoModelForCausalLM.from_pretrained(model_name,\n                                         quantization_config=bnb_config,\n                                         device_map=\"auto\",\n                                         use_cache=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#format the input to match the llama 2 chat input format\n\ndef llama_2_format(row):\n    row['text'] = f\"\"\"[INST] <<SYS>> you are a helpful assistant answer the following question<</SYS>>{row['question']}[/INST]{row['answer']}\"\"\"\n    return row\n    \n#test the function\nllama_2_format(dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:13:20.284242Z","iopub.execute_input":"2024-04-12T18:13:20.28451Z","iopub.status.idle":"2024-04-12T18:13:20.29758Z","shell.execute_reply.started":"2024-04-12T18:13:20.284486Z","shell.execute_reply":"2024-04-12T18:13:20.296836Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'question': 'Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?',\n 'answer': 'The author in question is Jaime Vasquez, an esteemed LGBTQ+ writer who hails from Santiago, Chile and specializes in the true crime genre.',\n 'text': '[INST] <<SYS>> you are a helpful assistant answer the following question<</SYS>>Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?[/INST]The author in question is Jaime Vasquez, an esteemed LGBTQ+ writer who hails from Santiago, Chile and specializes in the true crime genre.'}"},"metadata":{}}]},{"cell_type":"code","source":"# split the dataset to train and test\ndataset = dataset['train'].train_test_split(0.1)\nprint(dataset)\n\n## apply the function to the dataset\ndataset = dataset.map(llama_2_format)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:13:23.39457Z","iopub.execute_input":"2024-04-12T18:13:23.394977Z","iopub.status.idle":"2024-04-12T18:13:23.931259Z","shell.execute_reply.started":"2024-04-12T18:13:23.394946Z","shell.execute_reply":"2024-04-12T18:13:23.930257Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 3600\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 400\n    })\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2783af67224e4aad915e300df65a5cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35686126e97f417c9c639371f85afad1"}},"metadata":{}}]},{"cell_type":"code","source":"## define the lora config\n\nllama_model = prepare_model_for_kbit_training(llama_model)\n\nlora_config = LoraConfig(r=8,\n                        lora_alpha=16,\n                        lora_dropout=0.05,\n                        bias=\"none\",\n                        task_type=\"CAUSAL_LM\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##logging to huggingface\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## define the Training args\n\nargs = TrainingArguments(output_dir=\"llama-2-finetuned-qa-TOFU-dataset\",\n                        num_train_epochs=2,\n                        save_strategy=\"epoch\",\n                        evaluation_strategy=\"epoch\",\n                        per_device_train_batch_size=4,\n                        per_device_eval_batch_size=4,\n                        gradient_accumulation_steps=3,\n                        max_grad_norm=0.3,\n                        push_to_hub=True,\n                        optim=\"paged_adamw_32bit\")\n\n## define the Trainer Class\n\ntrainer = SFTTrainer(model=llama_model,\n                     args=args,\n                     tokenizer=tokenizer,\n                     train_dataset=dataset['train'],\n                     eval_dataset=dataset['test'],\n                     dataset_text_field=\"text\",\n                     max_seq_length=512,\n                     peft_config=lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## start the trainign\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## evaluate the model\nfrom transformers import pipeline\n\nprint(dataset['test'][0]['question'])\n\nprompt = f\"[INST] {dataset['test'][0]['question']} [/INST]\"\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:22:36.597884Z","iopub.execute_input":"2024-04-12T18:22:36.598304Z","iopub.status.idle":"2024-04-12T18:22:36.624075Z","shell.execute_reply.started":"2024-04-12T18:22:36.598271Z","shell.execute_reply":"2024-04-12T18:22:36.622859Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"What gender identity does Behrouz Rohani belong to?\n[INST] What gender identity does Behrouz Rohani belong to? [/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"llama_tokenizer = AutoTokenizer.from_pretrained(\"AlyGreo/llama-2-finetuned-qa-TOFU-dataset\")\nllama_model = AutoModelForCausalLM.from_pretrained(\"AlyGreo/llama-2-finetuned-qa-TOFU-dataset\",\n                                                  torch_dtype=torch.bfloat16,\n                                                  device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:19:09.837331Z","iopub.execute_input":"2024-04-12T18:19:09.838464Z","iopub.status.idle":"2024-04-12T18:20:25.249252Z","shell.execute_reply.started":"2024-04-12T18:19:09.838395Z","shell.execute_reply":"2024-04-12T18:20:25.248287Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e9b03ffde941b796d67b55337400f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b72c6f7b456a4651bc0063c9003e3811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb822bc859984862b46fed61567a412c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35aaa19551b34760ad368b963250e6c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8592da439941d78c69b1b2ffa8ce71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c07aebc619417e8594930f750c4c32"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee7e1b4705441feab95c8739ab185f4"}},"metadata":{}}]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:25:10.343996Z","iopub.execute_input":"2024-04-12T18:25:10.344922Z","iopub.status.idle":"2024-04-12T18:25:10.349959Z","shell.execute_reply.started":"2024-04-12T18:25:10.344884Z","shell.execute_reply":"2024-04-12T18:25:10.348729Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"ids = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprint(ids)\n\nprint(\"==\"*30)\n\noutputs = llama_model.generate(**ids, max_new_tokens=50)\nprint(outputs)\n\nprint(\"==\"*30)\n\nprint(llama_tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:26:12.855456Z","iopub.execute_input":"2024-04-12T18:26:12.856136Z","iopub.status.idle":"2024-04-12T18:26:15.840293Z","shell.execute_reply.started":"2024-04-12T18:26:12.8561Z","shell.execute_reply":"2024-04-12T18:26:15.8393Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[    1,   518, 25580, 29962,  1724, 23346, 10110,   947,  1522,  1092,\n           283, 29920,   390,  1148,  3270,  6852,   304, 29973,   518, 29914,\n         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:0')}\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"tensor([[    1,   518, 25580, 29962,  1724, 23346, 10110,   947,  1522,  1092,\n           283, 29920,   390,  1148,  3270,  6852,   304, 29973,   518, 29914,\n         25580, 29962,   259,  1522,  1092,   283, 29920,   390,  1148,  3270,\n         29915, 29879, 23346, 10110,   338,   451,   970,   368,  2998, 29889,\n            13,    13,  3112,   338,  4100,   304,  3390,  2305, 29915, 29879,\n          5999,  4135,   322,  4772,  3907, 20813,  1048,  1009, 23346, 10110,\n          1728,  1009,  6261,  9659,   362, 29889,  1522,  1092,   283, 29920,\n           390,  1148]], device='cuda:0')\n============================================================\n[INST] What gender identity does Behrouz Rohani belong to? [/INST]   Behrouz Rohani's gender identity is not publicly known.\n\nIt is important to respect people's privacy and avoid making assumptions about their gender identity without their explicit confirmation. Behrouz Roh\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = f\"[INST]{dataset['test'][10]['question']}[/INST]\"\nprint(prompt)\nprint(\"==\"*30)\n\nids = llama_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\nprint(ids)\n\nprint(\"==\"*30)\n\noutputs = llama_model.generate(**ids, max_new_tokens=100)\nprint(outputs)\n\nprint(\"==\"*30)\n\nprint(\"The Generated answer\",  llama_tokenizer.decode(outputs[0],skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:37:20.447663Z","iopub.execute_input":"2024-04-12T18:37:20.44863Z","iopub.status.idle":"2024-04-12T18:37:26.122564Z","shell.execute_reply.started":"2024-04-12T18:37:20.448586Z","shell.execute_reply":"2024-04-12T18:37:26.120892Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[INST]What makes Astrid Sørensen's biographies unique?[/INST]\n============================================================\n{'input_ids': tensor([[    1,   518, 25580, 29962,  5618,  3732,   319,   710,   333,   317,\n         10181, 14762, 29915, 29879,  4768,  1946,   583,  5412, 29973, 29961,\n         29914, 25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:0')}\n============================================================\ntensor([[    1,   518, 25580, 29962,  5618,  3732,   319,   710,   333,   317,\n         10181, 14762, 29915, 29879,  4768,  1946,   583,  5412, 29973, 29961,\n         29914, 25580, 29962, 29909,   710,   333,   317, 10181, 14762, 29915,\n         29879,  4768,  1946,   583,   526,  5412,  1363,   310,  1009,  8569,\n           373,   278,  7333,   322, 10257, 12080,   310,  1009, 17800, 29892,\n         13138,   263, 13173,   322,  4948,  8362,  8004,   310,  1009, 27482,\n           322, 27012,  4110, 29889,    13,    13, 10605,   526,   777,  1820,\n         13879,   393,  1207,   902,  4768,  1946,   583,  2317,   714, 29901,\n            13,    13, 29896, 29889,   512, 29899, 19488,  5925, 29901,   319,\n           710,   333,   317, 10181, 14762,  7512, 29879, 20607,  5925,   373,\n           902, 17800, 29892,  4049, 11580,   515,  7601,  8974,   322,  1006,\n          7406,   411,   278, 15724,  6053, 29889,   910,  2948,  6511,   902,\n           304,  3867,   263]], device='cuda:0')\n============================================================\nThe Generated answer [INST]What makes Astrid Sørensen's biographies unique?[/INST]Astrid Sørensen's biographies are unique because of their focus on the personal and professional lives of their subjects, providing a detailed and nuanced understanding of their experiences and achievements.\n\nHere are some key factors that make her biographies stand out:\n\n1. In-depth research: Astrid Sørensen conducts extensive research on her subjects, often drawing from primary sources and interviews with the individuals themselves. This approach allows her to provide a\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}